{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f3530e7-c261-498f-946b-67bcda94d41b",
   "metadata": {},
   "source": [
    "# MLFlow hands-on\n",
    "\n",
    "In this session we will take what we learned previously and add MLFlow configuration to already existing project.\n",
    "\n",
    "We will\n",
    "- look into the existing project\n",
    "- finish partially filled MLproject file\n",
    "- calculate predictions for csv file using model server "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2da9c3-af21-449c-8eb6-0d450af8f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "source ./mlflow_env_vars.sh\n",
    "\n",
    "mkdir -p data\n",
    "mlflow server --host 0.0.0.0 \\\n",
    "    --port 5000 \\\n",
    "    --backend-store-uri sqlite:///mlflow.db \\\n",
    "    --default-artifact-root ./mlruns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bdb142-96f2-4db8-8685-6c6c691a0519",
   "metadata": {},
   "source": [
    "## 1. The project\n",
    "\n",
    "In real-world scenarios MLOps will consist of productionizing existing projects.\n",
    "\n",
    "We have a project that uses [lending club dataset](https://www.kaggle.com/datasets/wordsforthewise/lending-club) for credit default risk.\n",
    "\n",
    "The scripts for downloading, preprocessing data and training are already implemented.\n",
    "\n",
    "Before running it, we need to set up a kaggle token. To do that, follow the instructions provided in [here](https://adityashrm21.github.io/Setting-Up-Kaggle/)\n",
    "\n",
    "Our task is supervised learning - we try to predict whether a loan was repaid. We have over 100 features that can be used for training a model.\n",
    "\n",
    "You will need to have high-level understanding of the code. That means mostly understanding function signatures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa6216c-e3d0-4f09-9274-4ae18f474f0f",
   "metadata": {},
   "source": [
    "Find appropriate entry point in `MLproject` and run the following cell to download data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed3f0ee-fc9f-47aa-9ed4-fc18a890da94",
   "metadata": {},
   "source": [
    "# 1. 0 Python Fire\n",
    "\n",
    "We will be using `fire`, a Python package for making convenient CLI apps from Python scripts, like `argparse` but smarter.\n",
    "\n",
    "`fire` works by wrapping Python script functions and exposing them to command line. \n",
    "\n",
    "\n",
    "`script.py:`\n",
    "\n",
    "```\n",
    "import fire\n",
    "\n",
    "def f(msg):\n",
    "    ...\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire()\n",
    "```\n",
    "\n",
    "Here it will infer that `f` is a command.\n",
    "You can call this in Python using `python script.py f $msg` and it will call `f(msg)` in Python.\n",
    "\n",
    "We will use `fire` in our scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1176ad17-af8c-4483-b6b5-39e69fc0591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only if Added Kaggle token (see README)\n",
    "# %%bash\n",
    "# source ./mlflow_env_vars.sh\n",
    "\n",
    "# mlflow run . -e download_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f51c259-4b2d-4ef5-ad53-b286fc1a8cc0",
   "metadata": {},
   "source": [
    "# 1. 1 Prepare data\n",
    "\n",
    "The following function is used to filter input data and define target. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026c4ed2-cac9-4703-bbff-8fde45e07f46",
   "metadata": {},
   "source": [
    "```python\n",
    "def filter_input_data(\n",
    "    data_path: str, # load csv file from this path\n",
    "    target_col: str, # the column that will be treated as target and transformed accordingly\n",
    "    dropped_values: List[str], # drop rows with target values like these \n",
    "    replace_by_zero: str, # replace target with this value as zero, other as 1\n",
    "    max_nan_proportion: float, # drop columns with more than this amount of NaN\n",
    "    max_categorical_cardinality: int, # drop categorical columns with more than this number of levels\n",
    "    dst_filename: str, # output filename (with feather extension)\n",
    "):\n",
    "    csv_path = data_path\n",
    "    df = pd.read_csv(csv_path)\n",
    "    cleaned_df = cleaning.filter_df(\n",
    "        df,\n",
    "        target_col,\n",
    "        dropped_values,\n",
    "        replace_by_zero,\n",
    "        max_nan_proportion,\n",
    "        max_categorical_cardinality,\n",
    "    )\n",
    "    cleaned_df.reset_index().to_parquet(str(dst_filename), index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca148a-40eb-456f-9000-e973e9d7596b",
   "metadata": {},
   "source": [
    "Implement `prepare_data` entrypoint in MLproject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73936e3-cfdf-4366-9ddf-2e16165a20e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import sys; print(sys.executable)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72048189-636b-4ed8-8a68-f3f35641fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of !pip install pandas\n",
    "# called conda install pandas in virtualenv mlops-student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96afcb62-2a20-4d39-966c-5d56c8226fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python -c \"import sys; print(sys.executable)\"\n",
    "source ./mlflow_env_vars.sh\n",
    "\n",
    "mlflow run . -e prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c3e6d9-ee2f-4603-9d5a-50618000152c",
   "metadata": {},
   "source": [
    "## 1. 2 Prepare train-test split\n",
    "\n",
    "Implement `prepare_train_test_split` in `MLproject` and use them to prepare two datasets containing records from different time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4341a774-64da-494d-b3a0-ce164fa43073",
   "metadata": {},
   "source": [
    "```python\n",
    "def prepare_train_test_split(data_path, seed, test_size, train_path, test_path):\n",
    "    df = utils.read_parquet(data_path)\n",
    "    train_df, test_df = model_selection.train_test_split(\n",
    "        df, test_size=test_size, random_state=seed\n",
    "    )\n",
    "    train_df.to_parquet(train_path, index=False)\n",
    "    test_df.to_parquet(test_path, index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbca501a-3583-41fa-9881-0784200ed57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./mlflow_env_vars.sh\n",
    "\n",
    "mlflow run . -e prepare_train_test_split_older\n",
    "mlflow run . -e prepare_train_test_split_newer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72d395-7846-447b-909d-c805c9fb2a74",
   "metadata": {},
   "source": [
    "## 1.3 Model training\n",
    "\n",
    "`train_model` trains appropriate model. You do not need to know all the details of training here.\n",
    "\n",
    "`model_conf.yaml` contains model pipeline configuration.\n",
    "\n",
    "Fill in the details in `main` entrypoint in `MLproject`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ad111a-a084-4b5b-82dd-3fc11558c2dc",
   "metadata": {},
   "source": [
    "```python\n",
    "def train_model(data_dir, config_dict_path):\n",
    "    with open(config_dict_path, \"r\") as f:\n",
    "        config_dict = yaml.safe_load(f)\n",
    "    config = configs.PipelineConfig(**config_dict)\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(config_dict)\n",
    "        logging.info(f\"training model: {config_dict}\")\n",
    "        clf_pipeline = pipelines.get_classification_pipeline(config)\n",
    "        X_train, y_train = prepare_input(data_dir, \"train\")\n",
    "        clf_pipeline.fit(X_train, y_train)\n",
    "        mlflow.sklearn.log_model(\n",
    "            clf_pipeline, \"model\", registered_model_name=\"ChurnModel\"\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c428dd-75c9-4968-bcce-4e43960ef309",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./mlflow_env_vars.sh\n",
    "mlflow run . -e main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3047aad-3cec-4761-9cd8-cd7fd22c1eb8",
   "metadata": {},
   "source": [
    "## Using trained model\n",
    "\n",
    "Recall that MLFLow models can be loaded in Python using several interfaces (scikit-learn, Keras et c)\n",
    "\n",
    "Our scikit-learn model can be loaded for example using MLFlow `pyfunc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c55be70-df0e-43be-ad33-f1f026b7fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow.pyfunc\n",
    "import pandas as pd\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23dcf03-cd54-456c-8561-78d240addee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://0.0.0.0:5000\"\n",
    "model_name = \"ChurnModel\"\n",
    "model_version_uri_1_0 = f\"models:/{model_name}_1_0/latest\"\n",
    "\n",
    "model_1_0 = mlflow.pyfunc.load_model(model_version_uri_1_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1becc386-24de-4385-8341-c74970b817dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_0 = mlflow.pyfunc.load_model(model_version_uri_1_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccfd14e-b058-42d9-9ddc-486f2ccdc643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_0 = pd.read_parquet(\"data/test1_0.parquet\")\n",
    "df_1_1 = pd.read_parquet(\"data/test1_1.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5718800e-1c93-4de0-acae-6900a2ebc718",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We have a binary classification problem that is somewhat imbalanced.\n",
    "\n",
    "The following cell will show the proportion of positive examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739b14b-f69b-4db5-b1e8-a434299d3baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_0[\"target\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254bb88b-3221-4c4e-accd-30bbc2a4ce14",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "#### Question 1: what are some metrics that are better suited than accuracy for imbalanced problems?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4133fcee-ec6d-4913-9181-1c3102d6e0ce",
   "metadata": {},
   "source": [
    "Answer: F1 score, recall, precision, ROC AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba0e471-6ef9-4081-ac75-c01ab194f710",
   "metadata": {},
   "source": [
    "#### Question 2: what is the problem with scores different from AUC?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8527a4a6-367f-45bf-b098-f08d7124ceb4",
   "metadata": {},
   "source": [
    "Answer: for example recall will measure how many positive examples have predicted $p(x) > 0.5$.\n",
    "\n",
    "In imbalanced problems this is an issue since $p(x)$ on average will be close to $y$ average (for example 15% in our case), so many positives would not have $p(x) > 0.5$.\n",
    "\n",
    "The solution to this is to use a score that does some kind of averaging over thresholds.\n",
    "\n",
    "**ROC AUC** is one such score that is commonly used in imbalanced problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17ac07-3ec9-4ce0-9e2e-39f1674335be",
   "metadata": {},
   "source": [
    "We will use this score now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e076cc-3231-4332-b352-d16e638db316",
   "metadata": {},
   "source": [
    "Run old model on old test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece9a528-b504-4057-9b39-4a7a6aae7f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./mlflow_env_vars.sh\n",
    "mlflow run . -e evaluate_model -P \"data_version='\"1_0\"'\" -P \"model_version='\"1_0\"'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fdfcc7-b022-48a4-afea-ab20890e57ad",
   "metadata": {},
   "source": [
    "Run old model on new test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeddb093-238a-4d2b-b39d-738b0bf41ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./mlflow_env_vars.sh\n",
    "mlflow run . -e evaluate_model -P \"data_version='\"1_1\"'\" -P \"model_version='\"1_0\"'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27efcdc5-cf3a-4a9c-8a5b-557e4bd87a0e",
   "metadata": {},
   "source": [
    "We see that the performance of model declined on new data.\n",
    "\n",
    "Let's train model on new data and see its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49811038-3544-47d4-898c-0f6be7dd3bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./mlflow_env_vars.sh\n",
    "mlflow run . -e main -P data_version=\"'1_1'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4776d501-6d12-42d8-9461-8feca4b5e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./mlflow_env_vars.sh\n",
    "mlflow run . -e evaluate_model -P \"data_version='\"1_1\"'\" -P \"model_version='\"1_1\"'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd088e38-250d-46c3-ab2b-32e75effe4f1",
   "metadata": {},
   "source": [
    "We see that the newer model is actually better on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46219ae8-46fe-4f0e-9176-5f5ddb9f7087",
   "metadata": {},
   "source": [
    "## Serving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fadfa6-761f-4917-9ad1-b21e00f1d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "source ./mlflow_env_vars.sh\n",
    "\n",
    "mlflow models serve -m models:/ChurnModel_1_1/Production -p 5001 --env-manager=conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c3acde-62cc-487b-a7e0-9a1f7a2a077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "source ./mlflow_env_vars.sh\n",
    "mlflow models serve -m models:/ChurnModel_1_1/Production -p 5001 --env-manager=conda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d903af5-e18d-4f62-a300-6afddaadbcb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prediction\n",
    "\n",
    "We'll load data that we can feed into prediction server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07cb7d6-98bb-4631-9adc-467d394beb11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
